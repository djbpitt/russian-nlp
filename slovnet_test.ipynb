{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c99144db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: natasha in /Applications/anaconda3/lib/python3.9/site-packages (1.4.0)\n",
      "Requirement already satisfied: pymorphy2 in /Applications/anaconda3/lib/python3.9/site-packages (from natasha) (0.9.1)\n",
      "Requirement already satisfied: yargy>=0.14.0 in /Applications/anaconda3/lib/python3.9/site-packages (from natasha) (0.15.0)\n",
      "Requirement already satisfied: slovnet>=0.3.0 in /Applications/anaconda3/lib/python3.9/site-packages (from natasha) (0.5.0)\n",
      "Requirement already satisfied: navec>=0.9.0 in /Applications/anaconda3/lib/python3.9/site-packages (from natasha) (0.10.0)\n",
      "Requirement already satisfied: ipymarkup>=0.8.0 in /Applications/anaconda3/lib/python3.9/site-packages (from natasha) (0.9.0)\n",
      "Requirement already satisfied: razdel>=0.5.0 in /Applications/anaconda3/lib/python3.9/site-packages (from natasha) (0.5.0)\n",
      "Requirement already satisfied: intervaltree>=3 in /Applications/anaconda3/lib/python3.9/site-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /Applications/anaconda3/lib/python3.9/site-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
      "Requirement already satisfied: numpy in /Users/Peter/.local/lib/python3.9/site-packages (from navec>=0.9.0->natasha) (1.22.3)\n",
      "Requirement already satisfied: docopt>=0.6 in /Applications/anaconda3/lib/python3.9/site-packages (from pymorphy2->natasha) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /Applications/anaconda3/lib/python3.9/site-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /Applications/anaconda3/lib/python3.9/site-packages (from pymorphy2->natasha) (0.7.2)\n",
      "Requirement already satisfied: natsort in /Applications/anaconda3/lib/python3.9/site-packages (8.1.0)\n",
      "Requirement already satisfied: navec in /Applications/anaconda3/lib/python3.9/site-packages (0.10.0)\n",
      "Requirement already satisfied: numpy in /Users/Peter/.local/lib/python3.9/site-packages (from navec) (1.22.3)\n"
     ]
    }
   ],
   "source": [
    "#Razdel — сегментация текста на предложения и токены;\n",
    "#Navec — качественный компактные эмбеддинги;\n",
    "#Slovnet — современные компактные модели для морфологии, синтаксиса, NER;\n",
    "#Yargy — правила и словари для извлечения структурированной информации;\n",
    "#Ipymarkup — визуализация NER и синтаксической разметки;\n",
    "!pip install natasha\n",
    "!pip install natsort\n",
    "!pip install navec\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "from natsort import natsorted\n",
    "\n",
    "import ipymarkup\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    MorphVocab,   \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    NewsNERTagger, \n",
    "    PER,\n",
    "    NamesExtractor,\n",
    "    Doc\n",
    ")\n",
    "\n",
    "segmenter = Segmenter()\n",
    "morph_vocab = MorphVocab()\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "ner_tagger = NewsNERTagger(emb)\n",
    "names_extractor = NamesExtractor(morph_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3274394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filepaths(d):\n",
    "    # List which will store all of the full filepaths.\n",
    "    locations = []\n",
    "    # os.walk to get tree info\n",
    "    for root, directories, files in os.walk(d):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".pkl\") and filename.startswith('ch'):\n",
    "            # join the strings to form filepath\n",
    "                filepath = os.path.join(root, filename)\n",
    "                locations.append(filepath)\n",
    "    # uses natsort to return human-intelligible ordering\n",
    "    return natsorted(locations)\n",
    "\n",
    "# store function results in a variable.   \n",
    "sorted_filepaths = get_filepaths('/Users/Peter/Documents/GitHub/DSAM2019/')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "609e24f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocToken(stop=6, text='Москва', pos='PROPN', feats=<Inan,Nom,Fem,Sing>)\n",
      "DocToken(stop=1, text='В', pos='ADP')\n",
      "DocToken(stop=1, text='–', pos='PUNCT')\n",
      "DocToken(stop=6, text='Утихли', pos='VERB', feats=<Perf,Ind,Plur,Past,Fin,Mid>)\n",
      "DocToken(stop=9, text='Старинный', pos='ADJ', feats=<Nom,Pos,Masc,Sing>)\n",
      "DocToken(stop=5, text='Когда', pos='SCONJ')\n",
      "DocToken(stop=4, text='Если', pos='SCONJ')\n",
      "DocToken(stop=3, text='Как', pos='SCONJ')\n",
      "DocToken(stop=7, text='Никанор', pos='PROPN', feats=<Anim,Nom,Masc,Sing>)\n",
      "DocToken(stop=1, text='В', pos='ADP')\n",
      "DocToken(stop=3, text='Бор', pos='PROPN', feats=<Inan,Acc,Masc,Sing>)\n",
      "DocToken(stop=9, text='Маленький', pos='ADJ', feats=<Nom,Pos,Masc,Sing>)\n",
      "DocToken(stop=4, text='Итак', pos='ADV', feats=<Pos>)\n",
      "DocToken(stop=2, text='Не', pos='PART', feats=<Neg>)\n",
      "DocToken(stop=8, text='Нетрудно', pos='PRON', feats=<Gen>)\n",
      "DocToken(stop=6, text='Солнце', pos='PROPN', feats=<Inan,Nom,Neut,Sing>)\n",
      "DocToken(stop=5, text='Утром', pos='NOUN', feats=<Inan,Ins,Neut,Sing>)\n",
      "DocToken(stop=1, text='В', pos='ADP')\n",
      "DocToken(stop=2, text='За', pos='ADP')\n",
      "DocToken(stop=4, text='Луна', pos='PROPN', feats=<Inan,Nom,Fem,Sing>)\n",
      "DocToken(stop=8, text='Невидима', pos='PROPN', feats=<Anim,Nom,Fem,Sing>)\n",
      "DocToken(stop=6, text='Ровное', pos='ADJ', feats=<Inan,Acc,Pos,Neut,Sing>)\n",
      "DocToken(stop=7, text='Полночь', pos='ADV', feats=<Pos>)\n",
      "DocToken(stop=1, text='В', pos='ADP')\n",
      "DocToken(stop=4, text='Тьма', pos='NOUN', feats=<Inan,Nom,Fem,Sing>)\n",
      "DocToken(stop=5, text='Может', pos='VERB', feats=<Imp,Ind,Sing,3,Pres,Fin,Act>)\n",
      "DocToken(stop=5, text='Когда', pos='SCONJ')\n",
      "DocToken(stop=4, text='Были', pos='VERB', feats=<Imp,Ind,Plur,Past,Fin,Act>)\n",
      "DocToken(stop=2, text='На', pos='ADP')\n",
      "DocToken(stop=1, text='–', pos='PUNCT')\n",
      "DocToken(stop=5, text='Грозу', pos='PROPN', feats=<Inan,Acc,Fem,Sing>)\n",
      "DocToken(stop=4, text='Боги', pos='ADJ', feats=<Nom,Pos,Plur>)\n",
      "DocToken(stop=2, text='Но', pos='CCONJ')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor span in doc.spans:\\n    if span.type == PER:\\n        span.extract_fact(names_extractor)\\n\\nfor span in doc.spans:\\n    span.normalize(morph_vocab)\\n    \\nfor span in doc.spans:\\n    if span.type == PER:\\n        span.extract_fact(names_extractor)\\n        \\ndates_extractor, money_extractor and addr_extractor\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wholebook = []\n",
    "for f in sorted_filepaths:\n",
    "    with open(str(f), 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        ch = ''.join(data)\n",
    "        wholebook.append(ch)\n",
    "        f.close()\n",
    "    \n",
    "for f in wholebook:\n",
    "    doc = Doc(f)\n",
    "    # divides doc into tokens and sents, given start and stop properties\n",
    "    doc.segment(segmenter)\n",
    "    # every token is morphologically tagged, given pos and feats properties\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    # named entity recognition\n",
    "    doc.tag_ner(ner_tagger)\n",
    "    #doc.sents[0].morph.print()\n",
    "    print(doc.tokens[0])\n",
    "    #print(doc.sents[0])\n",
    "    #print(\"\\n\")\n",
    "\n",
    "'''\n",
    "for span in doc.spans:\n",
    "    if span.type == PER:\n",
    "        span.extract_fact(names_extractor)\n",
    "\n",
    "for span in doc.spans:\n",
    "    span.normalize(morph_vocab)\n",
    "    \n",
    "for span in doc.spans:\n",
    "    if span.type == PER:\n",
    "        span.extract_fact(names_extractor)\n",
    "        \n",
    "dates_extractor, money_extractor and addr_extractor\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d38c1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocToken(stop=6, text='Москва', pos='PROPN', feats=<Inan,Nom,Fem,Sing>, lemma='москва')\n",
      "DocToken(stop=1, text='В', pos='ADP', lemma='в')\n",
      "DocToken(stop=1, text='–', pos='PUNCT', lemma='–')\n",
      "DocToken(stop=6, text='Утихли', pos='VERB', feats=<Perf,Ind,Plur,Past,Fin,Mid>, lemma='утихнуть')\n",
      "DocToken(stop=9, text='Старинный', pos='ADJ', feats=<Nom,Pos,Masc,Sing>, lemma='старинный')\n",
      "DocToken(stop=5, text='Когда', pos='SCONJ', lemma='когда')\n",
      "DocToken(stop=4, text='Если', pos='SCONJ', lemma='если')\n",
      "DocToken(stop=3, text='Как', pos='SCONJ', lemma='как')\n",
      "DocToken(stop=7, text='Никанор', pos='PROPN', feats=<Anim,Nom,Masc,Sing>, lemma='никанор')\n",
      "DocToken(stop=1, text='В', pos='ADP', lemma='в')\n",
      "DocToken(stop=3, text='Бор', pos='PROPN', feats=<Inan,Acc,Masc,Sing>, lemma='бор')\n",
      "DocToken(stop=9, text='Маленький', pos='ADJ', feats=<Nom,Pos,Masc,Sing>, lemma='маленький')\n",
      "DocToken(stop=4, text='Итак', pos='ADV', feats=<Pos>, lemma='итак')\n",
      "DocToken(stop=2, text='Не', pos='PART', feats=<Neg>, lemma='не')\n",
      "DocToken(stop=8, text='Нетрудно', pos='PRON', feats=<Gen>, lemma='нетрудно')\n",
      "DocToken(stop=6, text='Солнце', pos='PROPN', feats=<Inan,Nom,Neut,Sing>, lemma='солнце')\n",
      "DocToken(stop=5, text='Утром', pos='NOUN', feats=<Inan,Ins,Neut,Sing>, lemma='утро')\n",
      "DocToken(stop=1, text='В', pos='ADP', lemma='в')\n",
      "DocToken(stop=2, text='За', pos='ADP', lemma='за')\n",
      "DocToken(stop=4, text='Луна', pos='PROPN', feats=<Inan,Nom,Fem,Sing>, lemma='луна')\n",
      "DocToken(stop=8, text='Невидима', pos='PROPN', feats=<Anim,Nom,Fem,Sing>, lemma='невидимый')\n",
      "DocToken(stop=6, text='Ровное', pos='ADJ', feats=<Inan,Acc,Pos,Neut,Sing>, lemma='ровный')\n",
      "DocToken(stop=7, text='Полночь', pos='ADV', feats=<Pos>, lemma='полночь')\n",
      "DocToken(stop=1, text='В', pos='ADP', lemma='в')\n",
      "DocToken(stop=4, text='Тьма', pos='NOUN', feats=<Inan,Nom,Fem,Sing>, lemma='тьма')\n",
      "DocToken(stop=5, text='Может', pos='VERB', feats=<Imp,Ind,Sing,3,Pres,Fin,Act>, lemma='мочь')\n",
      "DocToken(stop=5, text='Когда', pos='SCONJ', lemma='когда')\n",
      "DocToken(stop=4, text='Были', pos='VERB', feats=<Imp,Ind,Plur,Past,Fin,Act>, lemma='быть')\n",
      "DocToken(stop=2, text='На', pos='ADP', lemma='на')\n",
      "DocToken(stop=1, text='–', pos='PUNCT', lemma='–')\n",
      "DocToken(stop=5, text='Грозу', pos='PROPN', feats=<Inan,Acc,Fem,Sing>, lemma='гроза')\n",
      "DocToken(stop=4, text='Боги', pos='ADJ', feats=<Nom,Pos,Plur>, lemma='бог')\n",
      "DocToken(stop=2, text='Но', pos='CCONJ', lemma='но')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nsyntax.print() to visualize syntax markup\\n\\ndoc.parse_syntax(syntax_parser)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for f in wholebook:\n",
    "    doc = Doc(f)   \n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    # lemmatizer\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "    print(doc.tokens[0])\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "'''\n",
    "syntax.print() to visualize syntax markup\n",
    "\n",
    "doc.parse_syntax(syntax_parser)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9485feb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d904ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
